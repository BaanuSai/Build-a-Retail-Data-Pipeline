{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building a Retail Data Pipeline\n\nThis notebook demonstrates how to build a data pipeline for a retail dataset. We'll use PostgreSQL for the database, and pandas for data manipulation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Connect to the Database\nWe'll start by connecting to the PostgreSQL database and fetching the data."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Connection successful\n"
    }
   ],
   "source": "import pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\nimport urllib.parse\n\nhostname = 'localhost'\ndatabase = 'SQL DataCamp Building a Retail sata Pipeline'\nusername = 'postgres'\npwd = 'SaiSankar@123'\nport_id = 5432\n\ntry:\n    # Establish the connection using psycopg2\n    conn = psycopg2.connect(\n        host=hostname,\n        dbname=database,\n        user=username,\n        password=pwd,\n        port=port_id)\n\n    print(\"Connection successful\")\n\n    # URL encode the password\n    encoded_password = urllib.parse.quote(pwd)\n\n    # Create the SQLAlchemy engine\n    connection_string = f'postgresql://{username}:{encoded_password}@{hostname}:{port_id}/{database}'\n    engine = create_engine(connection_string)\n\nexcept Exception as error:\n    print(\"Connection failed\")\n    print(error)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load Additional Data\nNext, we'll load additional data from a Parquet file."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Tables in the database: ['grocery_sales']\n   index  Store_ID                     Date  Dept  Weekly_Sales\n0      0         1  2010-02-05T00:00:00.000     1      24924.50\n1      1         1  2010-02-05T00:00:00.000    26      11737.12\n2      2         1  2010-02-05T00:00:00.000    17      13223.76\n3      3         1  2010-02-05T00:00:00.000    45         37.44\n4      4         1  2010-02-05T00:00:00.000    28       1085.29\n       index  Store_ID                     Date  Dept  Weekly_Sales\n0          0         1  2010-02-05T00:00:00.000     1      24924.50\n1          1         1  2010-02-05T00:00:00.000    26      11737.12\n2          2         1  2010-02-05T00:00:00.000    17      13223.76\n3          3         1  2010-02-05T00:00:00.000    45         37.44\n4          4         1  2010-02-05T00:00:00.000    28       1085.29\n...      ...      <<OutputTruncated>>0        NaN        NaN         NaN           NaN   NaN   \n\n            Size  \n0       151315.0  \n1       151315.0  \n2       151315.0  \n3       151315.0  \n4       151315.0  \n...          ...  \n231517  203819.0  \n231518  203819.0  \n231519  203819.0  \n231520  203819.0  \n231521       NaN  \n\n[231522 rows x 13 columns]\n"
    }
   ],
   "source": "import pandas as pd\n\n# Load extra data from parquet file\nextra_data = pd.read_parquet(r'C:\\Users\\12145\\Documents\\SkillTrack\\Data Camp Project\\extra_data.parquet')\nprint(extra_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Display Columns\nWe'll display the columns of both the `grocery_sales` and `extra_data` DataFrames."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "grocery_sales columns: Index(['index', 'Store_ID', 'Date', 'Dept', 'Weekly_Sales'], dtype='object')\nextra_data columns: Index(['index', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1',\n       'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI',\n       'Unemployment', 'Type', 'Size'],\n      dtype='object')\n"
    }
   ],
   "source": "print(\"grocery_sales columns:\", grocery_sales.columns)\nprint(\"extra_data columns:\", extra_data.columns)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Combine and Save the Data\nWe'll combine the two DataFrames and save the result as a CSV file."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "clean_data saved successfully.\n"
    }
   ],
   "source": "grocery_sales['Month'] = pd.to_datetime(grocery_sales['Date']).dt.month\ngrocery_sales = grocery_sales[['Store_ID', 'Month', 'Dept', 'Weekly_Sales']]\n\nif 'IsHoliday' in extra_data.columns and 'CPI' in extra_data.columns and 'Unemployment' in extra_data.columns:\n        extra_data = extra_data[['IsHoliday', 'CPI', 'Unemployment']]\n        \n        # Concatenate the DataFrames\n        combined_data = pd.concat([grocery_sales, extra_data], axis=1)\n\n        # Save the combined data to a CSV file\n        combined_data.to_csv('clean_data.csv', index=False)\n        print(\"clean_data saved successfully.\")\nelse:\n        print(\"Required columns are missing in extra_data.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Visualize the Data\nLet's create some visualizations to better understand the data."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualize the correlation between Weekly_Sales and other features\nplt.figure(figsize=(12, 6))\nsns.heatmap(data_combined.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n# Scatter plot of Weekly_Sales vs Temperature\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Temperature', y='Weekly_Sales', data=data_combined)\nplt.title('Weekly Sales vs Temperature')\nplt.xlabel('Temperature')\nplt.ylabel('Weekly Sales')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary of Results\n\nBelow is a summary of the key statistics from the combined data."
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "              index      Store_ID          Dept   Weekly_Sales     IsHoliday  \\\ncount  20000.000000  20000.000000  20000.000000   19962.000000  20000.000000   \nmean   10018.524200      1.487800     44.437750   24272.506712      0.072250   \nstd     5790.533109      0.499864     29.926163   30580.043926      0.258908   \nmin        0.000000 &#8203;:citation[【oaicite:0】]&#8203;
